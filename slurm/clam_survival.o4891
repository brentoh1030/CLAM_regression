
Load Dataset
This is a survival prediction task.
Event time stats:
count    2027.000000
mean      474.619142
std       511.481073
min         0.000000
25%       178.000000
50%       335.000000
75%       588.000000
max      3881.000000
Name: event_time, dtype: float64
Censorship stats:
censorship
1    1617
0     410
Name: count, dtype: int64
split_dir:  /home/brentoh1030/workspace/CLAM/splits/task_3_survival_prediction_100
################# Settings ###################
num_splits:  10
k_start:  -1
k_end:  -1
task:  task_3_survival_prediction
task_type:  regression
max_epochs:  200
results_dir:  /home/brentoh1030/workspace/CLAM/results
lr:  0.0002
experiment:  task_3_survival_prediction_CLAM_50
reg:  1e-05
label_frac:  1.0
bag_loss:  ce
seed:  1
model_type:  clam_sb
model_size:  small
use_drop_out:  0.25
weighted_sample:  False
opt:  adam
bag_weight:  0.7
inst_loss:  None
B:  8
split_dir:  /home/brentoh1030/workspace/CLAM/splits/task_3_survival_prediction_100
Generated splits: Train size: 1623, Val size: 202, Test size: 202

Training Fold 0!

Init train/val/test splits... 
Done!
Training on 1623 samples
Validating on 202 samples
Testing on 202 samples

Init loss function... Done!

Init Model... Done!
CLAM_SB(
  (attention_net): Sequential(
    (0): Linear(in_features=1324, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=1, bias=True)
    )
  )
  (instance_classifiers): ModuleList()
  (instance_loss_fn): CrossEntropyLoss()
  (risk_layer): Linear(in_features=512, out_features=1, bias=True)
)
Total number of parameters: 941826
Total number of trainable parameters: 941826

Init optimizer ... Done!

Init Loaders... Done!

Setup EarlyStopping... Done!


Batch 32/1623, Avg Loss: 0.0831
Batch 64/1623, Avg Loss: 0.0818
Batch 96/1623, Avg Loss: 0.0817
Batch 128/1623, Avg Loss: 0.0814
Batch 160/1623, Avg Loss: 0.0815
Batch 192/1623, Avg Loss: 0.0809
Batch 224/1623, Avg Loss: 0.0816
Batch 256/1623, Avg Loss: 0.0813
Batch 288/1623, Avg Loss: 0.0808
Batch 320/1623, Avg Loss: 0.0811
Batch 352/1623, Avg Loss: 0.0812
Batch 384/1623, Avg Loss: 0.0810
Batch 416/1623, Avg Loss: 0.0805
Batch 448/1623, Avg Loss: 0.0803
Batch 480/1623, Avg Loss: 0.0801
Batch 512/1623, Avg Loss: 0.0804
Batch 544/1623, Avg Loss: 0.0801
Batch 576/1623, Avg Loss: 0.0797
Batch 608/1623, Avg Loss: 0.0796
Batch 640/1623, Avg Loss: 0.0794
Batch 672/1623, Avg Loss: 0.0794
Batch 704/1623, Avg Loss: 0.0794
Batch 736/1623, Avg Loss: 0.0792
Batch 768/1623, Avg Loss: 0.0790
Batch 800/1623, Avg Loss: 0.0792
Batch 832/1623, Avg Loss: 0.0794
Batch 864/1623, Avg Loss: 0.0794
Batch 896/1623, Avg Loss: 0.0793
Batch 928/1623, Avg Loss: 0.0793
Batch 960/1623, Avg Loss: 0.0793
Batch 992/1623, Avg Loss: 0.0792
Batch 1024/1623, Avg Loss: 0.0792
Batch 1056/1623, Avg Loss: 0.0791
Batch 1088/1623, Avg Loss: 0.0790
Batch 1120/1623, Avg Loss: 0.0789
Batch 1152/1623, Avg Loss: 0.0789
Batch 1184/1623, Avg Loss: 0.0789
Batch 1216/1623, Avg Loss: 0.0789
Batch 1248/1623, Avg Loss: 0.0789
Batch 1280/1623, Avg Loss: 0.0789
Batch 1312/1623, Avg Loss: 0.0790
Batch 1344/1623, Avg Loss: 0.0790
Batch 1376/1623, Avg Loss: 0.0789
Batch 1408/1623, Avg Loss: 0.0788
Batch 1440/1623, Avg Loss: 0.0787
Batch 1472/1623, Avg Loss: 0.0787
Batch 1504/1623, Avg Loss: 0.0787
Batch 1536/1623, Avg Loss: 0.0787
Batch 1568/1623, Avg Loss: 0.0785
Batch 1600/1623, Avg Loss: 0.0786
Batch 1623/1623, Avg Loss: 0.0788

Epoch 0: Train Loss: 0.0788

Val Set, val_loss: 4.3419, C-index: 0.4954
Validation loss decreased (inf --> 4.341875).  Saving model ...


Batch 32/1623, Avg Loss: 0.0753
Batch 64/1623, Avg Loss: 0.0770
Batch 96/1623, Avg Loss: 0.0764
Batch 128/1623, Avg Loss: 0.0763
Batch 160/1623, Avg Loss: 0.0765
Batch 192/1623, Avg Loss: 0.0759
Batch 224/1623, Avg Loss: 0.0764
Batch 256/1623, Avg Loss: 0.0763
Batch 288/1623, Avg Loss: 0.0767
Batch 320/1623, Avg Loss: 0.0765
Batch 352/1623, Avg Loss: 0.0767
Batch 384/1623, Avg Loss: 0.0769
Batch 416/1623, Avg Loss: 0.0768
Batch 448/1623, Avg Loss: 0.0769
Batch 480/1623, Avg Loss: 0.0767
Batch 512/1623, Avg Loss: 0.0769
Batch 544/1623, Avg Loss: 0.0771
Batch 576/1623, Avg Loss: 0.0771
Batch 608/1623, Avg Loss: 0.0769
Batch 640/1623, Avg Loss: 0.0769
Batch 672/1623, Avg Loss: 0.0772
Batch 704/1623, Avg Loss: 0.0771
Batch 736/1623, Avg Loss: 0.0772
Batch 768/1623, Avg Loss: 0.0771
Batch 800/1623, Avg Loss: 0.0770
Batch 832/1623, Avg Loss: 0.0772
Batch 864/1623, Avg Loss: 0.0772
Batch 896/1623, Avg Loss: 0.0771
Batch 928/1623, Avg Loss: 0.0771
Batch 960/1623, Avg Loss: 0.0772
Batch 992/1623, Avg Loss: 0.0772
Batch 1024/1623, Avg Loss: 0.0773
Batch 1056/1623, Avg Loss: 0.0773
Batch 1088/1623, Avg Loss: 0.0771
Batch 1120/1623, Avg Loss: 0.0771
Batch 1152/1623, Avg Loss: 0.0771
Batch 1184/1623, Avg Loss: 0.0773
Batch 1216/1623, Avg Loss: 0.0772
Batch 1248/1623, Avg Loss: 0.0773
Batch 1280/1623, Avg Loss: 0.0773
Batch 1312/1623, Avg Loss: 0.0772
Batch 1344/1623, Avg Loss: 0.0773
Batch 1376/1623, Avg Loss: 0.0772
Batch 1408/1623, Avg Loss: 0.0772
Batch 1440/1623, Avg Loss: 0.0772
Batch 1472/1623, Avg Loss: 0.0773
Batch 1504/1623, Avg Loss: 0.0772
Batch 1536/1623, Avg Loss: 0.0771
Batch 1568/1623, Avg Loss: 0.0771
Batch 1600/1623, Avg Loss: 0.0770
Batch 1623/1623, Avg Loss: 0.0772

Epoch 1: Train Loss: 0.0772

Val Set, val_loss: 4.2578, C-index: 0.4376
Validation loss decreased (4.341875 --> 4.257810).  Saving model ...


Batch 32/1623, Avg Loss: 0.0744
Batch 64/1623, Avg Loss: 0.0763
Batch 96/1623, Avg Loss: 0.0753
Batch 128/1623, Avg Loss: 0.0753
Batch 160/1623, Avg Loss: 0.0749
Batch 192/1623, Avg Loss: 0.0751
Batch 224/1623, Avg Loss: 0.0743
Batch 256/1623, Avg Loss: 0.0749
Batch 288/1623, Avg Loss: 0.0750
Batch 320/1623, Avg Loss: 0.0747
Batch 352/1623, Avg Loss: 0.0747
Batch 384/1623, Avg Loss: 0.0747
Batch 416/1623, Avg Loss: 0.0749
Batch 448/1623, Avg Loss: 0.0749
Batch 480/1623, Avg Loss: 0.0747
Batch 512/1623, Avg Loss: 0.0749
Batch 544/1623, Avg Loss: 0.0749
Batch 576/1623, Avg Loss: 0.0751
Batch 608/1623, Avg Loss: 0.0752
Batch 640/1623, Avg Loss: 0.0752
Batch 672/1623, Avg Loss: 0.0754
Batch 704/1623, Avg Loss: 0.0755
Batch 736/1623, Avg Loss: 0.0754
Batch 768/1623, Avg Loss: 0.0753
Batch 800/1623, Avg Loss: 0.0754
Batch 832/1623, Avg Loss: 0.0754
Batch 864/1623, Avg Loss: 0.0755
Batch 896/1623, Avg Loss: 0.0755
Batch 928/1623, Avg Loss: 0.0753
Batch 960/1623, Avg Loss: 0.0755
Batch 992/1623, Avg Loss: 0.0756
Batch 1024/1623, Avg Loss: 0.0756
Batch 1056/1623, Avg Loss: 0.0757
Batch 1088/1623, Avg Loss: 0.0757
Batch 1120/1623, Avg Loss: 0.0758
Batch 1152/1623, Avg Loss: 0.0757
Batch 1184/1623, Avg Loss: 0.0757
Batch 1216/1623, Avg Loss: 0.0759
Batch 1248/1623, Avg Loss: 0.0759
Batch 1280/1623, Avg Loss: 0.0760
Batch 1312/1623, Avg Loss: 0.0760
Batch 1344/1623, Avg Loss: 0.0760
Batch 1376/1623, Avg Loss: 0.0760
Batch 1408/1623, Avg Loss: 0.0760
Batch 1440/1623, Avg Loss: 0.0760
Batch 1472/1623, Avg Loss: 0.0760
Batch 1504/1623, Avg Loss: 0.0760
Batch 1536/1623, Avg Loss: 0.0760
Batch 1568/1623, Avg Loss: 0.0759
Batch 1600/1623, Avg Loss: 0.0760
Batch 1623/1623, Avg Loss: 0.0762

Epoch 2: Train Loss: 0.0762

Val Set, val_loss: 4.3419, C-index: 0.4766
EarlyStopping counter: 1 out of 20


Batch 32/1623, Avg Loss: 0.0798
Batch 64/1623, Avg Loss: 0.0739
Batch 96/1623, Avg Loss: 0.0741
Batch 128/1623, Avg Loss: 0.0727
Batch 160/1623, Avg Loss: 0.0737
Batch 192/1623, Avg Loss: 0.0735
Batch 224/1623, Avg Loss: 0.0736
Batch 256/1623, Avg Loss: 0.0737
Batch 288/1623, Avg Loss: 0.0735
Batch 320/1623, Avg Loss: 0.0732
Batch 352/1623, Avg Loss: 0.0737
Batch 384/1623, Avg Loss: 0.0736
Batch 416/1623, Avg Loss: 0.0739
Batch 448/1623, Avg Loss: 0.0736
Batch 480/1623, Avg Loss: 0.0735
Batch 512/1623, Avg Loss: 0.0736
Batch 544/1623, Avg Loss: 0.0739
Batch 576/1623, Avg Loss: 0.0737
Batch 608/1623, Avg Loss: 0.0738
Batch 640/1623, Avg Loss: 0.0739
Batch 672/1623, Avg Loss: 0.0741
Batch 704/1623, Avg Loss: 0.0743
Batch 736/1623, Avg Loss: 0.0744
Batch 768/1623, Avg Loss: 0.0746
Batch 800/1623, Avg Loss: 0.0747
Batch 832/1623, Avg Loss: 0.0748
Batch 864/1623, Avg Loss: 0.0750
Batch 896/1623, Avg Loss: 0.0751
Batch 928/1623, Avg Loss: 0.0750
Batch 960/1623, Avg Loss: 0.0749
Batch 992/1623, Avg Loss: 0.0749
Batch 1024/1623, Avg Loss: 0.0750
Batch 1056/1623, Avg Loss: 0.0749
Batch 1088/1623, Avg Loss: 0.0748
Batch 1120/1623, Avg Loss: 0.0747
Batch 1152/1623, Avg Loss: 0.0746
Batch 1184/1623, Avg Loss: 0.0746
Batch 1216/1623, Avg Loss: 0.0745
Batch 1248/1623, Avg Loss: 0.0744
Batch 1280/1623, Avg Loss: 0.0746
Batch 1312/1623, Avg Loss: 0.0746
Batch 1344/1623, Avg Loss: 0.0746
Batch 1376/1623, Avg Loss: 0.0746
Batch 1408/1623, Avg Loss: 0.0747
Batch 1440/1623, Avg Loss: 0.0748
Batch 1472/1623, Avg Loss: 0.0748
Batch 1504/1623, Avg Loss: 0.0748
Batch 1536/1623, Avg Loss: 0.0747
Batch 1568/1623, Avg Loss: 0.0748
Batch 1600/1623, Avg Loss: 0.0748
Batch 1623/1623, Avg Loss: 0.0750

Epoch 3: Train Loss: 0.0750

Val Set, val_loss: 4.4105, C-index: 0.4866
EarlyStopping counter: 2 out of 20


Batch 32/1623, Avg Loss: 0.0739
Batch 64/1623, Avg Loss: 0.0721
Batch 96/1623, Avg Loss: 0.0742
Batch 128/1623, Avg Loss: 0.0733
Batch 160/1623, Avg Loss: 0.0733
Batch 192/1623, Avg Loss: 0.0729
Batch 224/1623, Avg Loss: 0.0728
Batch 256/1623, Avg Loss: 0.0728
Batch 288/1623, Avg Loss: 0.0732
Batch 320/1623, Avg Loss: 0.0731
Batch 352/1623, Avg Loss: 0.0731
Batch 384/1623, Avg Loss: 0.0729
Batch 416/1623, Avg Loss: 0.0729
Batch 448/1623, Avg Loss: 0.0730
Batch 480/1623, Avg Loss: 0.0730
Batch 512/1623, Avg Loss: 0.0732
Batch 544/1623, Avg Loss: 0.0731
Batch 576/1623, Avg Loss: 0.0735
Batch 608/1623, Avg Loss: 0.0735
Batch 640/1623, Avg Loss: 0.0735
Batch 672/1623, Avg Loss: 0.0736
Batch 704/1623, Avg Loss: 0.0738
Batch 736/1623, Avg Loss: 0.0737
Batch 768/1623, Avg Loss: 0.0736
Batch 800/1623, Avg Loss: 0.0737
Batch 832/1623, Avg Loss: 0.0738
Batch 864/1623, Avg Loss: 0.0737
Batch 896/1623, Avg Loss: 0.0736
Batch 928/1623, Avg Loss: 0.0735
Batch 960/1623, Avg Loss: 0.0736
Batch 992/1623, Avg Loss: 0.0738
Batch 1024/1623, Avg Loss: 0.0738
Batch 1056/1623, Avg Loss: 0.0737
Batch 1088/1623, Avg Loss: 0.0736
Batch 1120/1623, Avg Loss: 0.0736
Batch 1152/1623, Avg Loss: 0.0737
Batch 1184/1623, Avg Loss: 0.0736
Batch 1216/1623, Avg Loss: 0.0736
Batch 1248/1623, Avg Loss: 0.0737
Batch 1280/1623, Avg Loss: 0.0737
Batch 1312/1623, Avg Loss: 0.0739
Batch 1344/1623, Avg Loss: 0.0738
Batch 1376/1623, Avg Loss: 0.0737
Batch 1408/1623, Avg Loss: 0.0735
Batch 1440/1623, Avg Loss: 0.0736
Batch 1472/1623, Avg Loss: 0.0736
Batch 1504/1623, Avg Loss: 0.0737
Batch 1536/1623, Avg Loss: 0.0738
Batch 1568/1623, Avg Loss: 0.0738
Batch 1600/1623, Avg Loss: 0.0740
Batch 1623/1623, Avg Loss: 0.0743

Epoch 4: Train Loss: 0.0743

Val Set, val_loss: 4.3920, C-index: 0.4652
EarlyStopping counter: 3 out of 20


Batch 32/1623, Avg Loss: 0.0691
Batch 64/1623, Avg Loss: 0.0718
Batch 96/1623, Avg Loss: 0.0711
Batch 128/1623, Avg Loss: 0.0725
Batch 160/1623, Avg Loss: 0.0728
Batch 192/1623, Avg Loss: 0.0724
Batch 224/1623, Avg Loss: 0.0727
Batch 256/1623, Avg Loss: 0.0724
Batch 288/1623, Avg Loss: 0.0724
Batch 320/1623, Avg Loss: 0.0719
Batch 352/1623, Avg Loss: 0.0715
Batch 384/1623, Avg Loss: 0.0716
Batch 416/1623, Avg Loss: 0.0720
Batch 448/1623, Avg Loss: 0.0722
Batch 480/1623, Avg Loss: 0.0719
Batch 512/1623, Avg Loss: 0.0720
Batch 544/1623, Avg Loss: 0.0719
Batch 576/1623, Avg Loss: 0.0717
Batch 608/1623, Avg Loss: 0.0721
Batch 640/1623, Avg Loss: 0.0721
Batch 672/1623, Avg Loss: 0.0721
Batch 704/1623, Avg Loss: 0.0722
Batch 736/1623, Avg Loss: 0.0722
Batch 768/1623, Avg Loss: 0.0721
Batch 800/1623, Avg Loss: 0.0721
Batch 832/1623, Avg Loss: 0.0720
Batch 864/1623, Avg Loss: 0.0721
Batch 896/1623, Avg Loss: 0.0721
Batch 928/1623, Avg Loss: 0.0721
Batch 960/1623, Avg Loss: 0.0723
Batch 992/1623, Avg Loss: 0.0723
Batch 1024/1623, Avg Loss: 0.0722
Batch 1056/1623, Avg Loss: 0.0722
Batch 1088/1623, Avg Loss: 0.0724
Batch 1120/1623, Avg Loss: 0.0726
Batch 1152/1623, Avg Loss: 0.0727
Batch 1184/1623, Avg Loss: 0.0728
Batch 1216/1623, Avg Loss: 0.0727
Batch 1248/1623, Avg Loss: 0.0728
Batch 1280/1623, Avg Loss: 0.0728
Batch 1312/1623, Avg Loss: 0.0729
Batch 1344/1623, Avg Loss: 0.0729
Batch 1376/1623, Avg Loss: 0.0729
Batch 1408/1623, Avg Loss: 0.0728
Batch 1440/1623, Avg Loss: 0.0729
Batch 1472/1623, Avg Loss: 0.0728
Batch 1504/1623, Avg Loss: 0.0727
Batch 1536/1623, Avg Loss: 0.0727
Batch 1568/1623, Avg Loss: 0.0727
Batch 1600/1623, Avg Loss: 0.0727
Batch 1623/1623, Avg Loss: 0.0728

Epoch 5: Train Loss: 0.0728

Val Set, val_loss: 4.3261, C-index: 0.4370
EarlyStopping counter: 4 out of 20


Batch 32/1623, Avg Loss: 0.0700
Batch 64/1623, Avg Loss: 0.0691
Batch 96/1623, Avg Loss: 0.0696
Batch 128/1623, Avg Loss: 0.0705
Batch 160/1623, Avg Loss: 0.0697
Batch 192/1623, Avg Loss: 0.0697
Batch 224/1623, Avg Loss: 0.0698
Batch 256/1623, Avg Loss: 0.0708
Batch 288/1623, Avg Loss: 0.0714
Batch 320/1623, Avg Loss: 0.0714
Batch 352/1623, Avg Loss: 0.0710
Batch 384/1623, Avg Loss: 0.0707
Batch 416/1623, Avg Loss: 0.0709
Batch 448/1623, Avg Loss: 0.0714
Batch 480/1623, Avg Loss: 0.0717
Batch 512/1623, Avg Loss: 0.0717
Batch 544/1623, Avg Loss: 0.0718
Batch 576/1623, Avg Loss: 0.0720
Batch 608/1623, Avg Loss: 0.0719
Batch 640/1623, Avg Loss: 0.0724
Batch 672/1623, Avg Loss: 0.0725
Batch 704/1623, Avg Loss: 0.0723
Batch 736/1623, Avg Loss: 0.0722
Batch 768/1623, Avg Loss: 0.0721
Batch 800/1623, Avg Loss: 0.0718
Batch 832/1623, Avg Loss: 0.0719
Batch 864/1623, Avg Loss: 0.0720
Batch 896/1623, Avg Loss: 0.0720
Batch 928/1623, Avg Loss: 0.0723
Batch 960/1623, Avg Loss: 0.0723
Batch 992/1623, Avg Loss: 0.0722
Batch 1024/1623, Avg Loss: 0.0722
Batch 1056/1623, Avg Loss: 0.0723
Batch 1088/1623, Avg Loss: 0.0723
Batch 1120/1623, Avg Loss: 0.0723
Batch 1152/1623, Avg Loss: 0.0724
Batch 1184/1623, Avg Loss: 0.0724
Batch 1216/1623, Avg Loss: 0.0724
Batch 1248/1623, Avg Loss: 0.0723
Batch 1280/1623, Avg Loss: 0.0722
Batch 1312/1623, Avg Loss: 0.0722
Batch 1344/1623, Avg Loss: 0.0722
Batch 1376/1623, Avg Loss: 0.0722
Batch 1408/1623, Avg Loss: 0.0722
Batch 1440/1623, Avg Loss: 0.0722
Batch 1472/1623, Avg Loss: 0.0722
Batch 1504/1623, Avg Loss: 0.0721
Batch 1536/1623, Avg Loss: 0.0721
Batch 1568/1623, Avg Loss: 0.0721
Batch 1600/1623, Avg Loss: 0.0721
Batch 1623/1623, Avg Loss: 0.0724

Epoch 6: Train Loss: 0.0724

Val Set, val_loss: 4.4130, C-index: 0.4439
EarlyStopping counter: 5 out of 20


Batch 32/1623, Avg Loss: 0.0803
Batch 64/1623, Avg Loss: 0.0742
Batch 96/1623, Avg Loss: 0.0731
Batch 128/1623, Avg Loss: 0.0737
Batch 160/1623, Avg Loss: 0.0720
Batch 192/1623, Avg Loss: 0.0715
Batch 224/1623, Avg Loss: 0.0720
Batch 256/1623, Avg Loss: 0.0721
Batch 288/1623, Avg Loss: 0.0725
Batch 320/1623, Avg Loss: 0.0732
Batch 352/1623, Avg Loss: 0.0728
Batch 384/1623, Avg Loss: 0.0728
Batch 416/1623, Avg Loss: 0.0722
Batch 448/1623, Avg Loss: 0.0721
Batch 480/1623, Avg Loss: 0.0715
Batch 512/1623, Avg Loss: 0.0713
Batch 544/1623, Avg Loss: 0.0711
Batch 576/1623, Avg Loss: 0.0711
Batch 608/1623, Avg Loss: 0.0708
Batch 640/1623, Avg Loss: 0.0712
Batch 672/1623, Avg Loss: 0.0712
Batch 704/1623, Avg Loss: 0.0713
Batch 736/1623, Avg Loss: 0.0712
Batch 768/1623, Avg Loss: 0.0711
Batch 800/1623, Avg Loss: 0.0712
Batch 832/1623, Avg Loss: 0.0716
Batch 864/1623, Avg Loss: 0.0715
Batch 896/1623, Avg Loss: 0.0714
Batch 928/1623, Avg Loss: 0.0715
Batch 960/1623, Avg Loss: 0.0714
Batch 992/1623, Avg Loss: 0.0714
Batch 1024/1623, Avg Loss: 0.0714
Batch 1056/1623, Avg Loss: 0.0715
Batch 1088/1623, Avg Loss: 0.0714
Batch 1120/1623, Avg Loss: 0.0713
Batch 1152/1623, Avg Loss: 0.0713
Batch 1184/1623, Avg Loss: 0.0713
Batch 1216/1623, Avg Loss: 0.0713
Batch 1248/1623, Avg Loss: 0.0713
Batch 1280/1623, Avg Loss: 0.0712
Batch 1312/1623, Avg Loss: 0.0711
Batch 1344/1623, Avg Loss: 0.0711
Batch 1376/1623, Avg Loss: 0.0711
Batch 1408/1623, Avg Loss: 0.0712
Batch 1440/1623, Avg Loss: 0.0711
Batch 1472/1623, Avg Loss: 0.0712
Batch 1504/1623, Avg Loss: 0.0712
Batch 1536/1623, Avg Loss: 0.0712
Batch 1568/1623, Avg Loss: 0.0713
Batch 1600/1623, Avg Loss: 0.0713
Batch 1623/1623, Avg Loss: 0.0715

Epoch 7: Train Loss: 0.0715

Val Set, val_loss: 4.3460, C-index: 0.4265
EarlyStopping counter: 6 out of 20


Batch 32/1623, Avg Loss: 0.0708
Batch 64/1623, Avg Loss: 0.0731
Batch 96/1623, Avg Loss: 0.0712
Batch 128/1623, Avg Loss: 0.0717
Batch 160/1623, Avg Loss: 0.0712
Batch 192/1623, Avg Loss: 0.0715
Batch 224/1623, Avg Loss: 0.0699
Batch 256/1623, Avg Loss: 0.0698
Batch 288/1623, Avg Loss: 0.0699
Batch 320/1623, Avg Loss: 0.0702
Batch 352/1623, Avg Loss: 0.0699
Batch 384/1623, Avg Loss: 0.0701
Batch 416/1623, Avg Loss: 0.0701
Batch 448/1623, Avg Loss: 0.0704
Batch 480/1623, Avg Loss: 0.0704
Batch 512/1623, Avg Loss: 0.0706
Batch 544/1623, Avg Loss: 0.0702
Batch 576/1623, Avg Loss: 0.0702
Batch 608/1623, Avg Loss: 0.0701
Batch 640/1623, Avg Loss: 0.0700
Batch 672/1623, Avg Loss: 0.0701
Batch 704/1623, Avg Loss: 0.0704
Batch 736/1623, Avg Loss: 0.0704
Batch 768/1623, Avg Loss: 0.0707
Batch 800/1623, Avg Loss: 0.0707
Batch 832/1623, Avg Loss: 0.0703
Batch 864/1623, Avg Loss: 0.0701
Batch 896/1623, Avg Loss: 0.0704
Batch 928/1623, Avg Loss: 0.0704
Batch 960/1623, Avg Loss: 0.0702
Batch 992/1623, Avg Loss: 0.0698
Batch 1024/1623, Avg Loss: 0.0698
Batch 1056/1623, Avg Loss: 0.0698
Batch 1088/1623, Avg Loss: 0.0700
Batch 1120/1623, Avg Loss: 0.0700
Batch 1152/1623, Avg Loss: 0.0700
Batch 1184/1623, Avg Loss: 0.0701
Batch 1216/1623, Avg Loss: 0.0700
Batch 1248/1623, Avg Loss: 0.0699
Batch 1280/1623, Avg Loss: 0.0698
Batch 1312/1623, Avg Loss: 0.0696
Batch 1344/1623, Avg Loss: 0.0695
Batch 1376/1623, Avg Loss: 0.0694
Batch 1408/1623, Avg Loss: 0.0693
Batch 1440/1623, Avg Loss: 0.0693
Batch 1472/1623, Avg Loss: 0.0696
Batch 1504/1623, Avg Loss: 0.0694
Batch 1536/1623, Avg Loss: 0.0695
Batch 1568/1623, Avg Loss: 0.0696
Batch 1600/1623, Avg Loss: 0.0696
Batch 1623/1623, Avg Loss: 0.0697

Epoch 8: Train Loss: 0.0697

Val Set, val_loss: 4.3909, C-index: 0.4188
EarlyStopping counter: 7 out of 20


Batch 32/1623, Avg Loss: 0.0677
Batch 64/1623, Avg Loss: 0.0686
Batch 96/1623, Avg Loss: 0.0695
Batch 128/1623, Avg Loss: 0.0689
Batch 160/1623, Avg Loss: 0.0675
Batch 192/1623, Avg Loss: 0.0687
Batch 224/1623, Avg Loss: 0.0688
Batch 256/1623, Avg Loss: 0.0689
Batch 288/1623, Avg Loss: 0.0678
Batch 320/1623, Avg Loss: 0.0680
Batch 352/1623, Avg Loss: 0.0683
Batch 384/1623, Avg Loss: 0.0686
Batch 416/1623, Avg Loss: 0.0684
Batch 448/1623, Avg Loss: 0.0685
Batch 480/1623, Avg Loss: 0.0685
Batch 512/1623, Avg Loss: 0.0684
Batch 544/1623, Avg Loss: 0.0685
Batch 576/1623, Avg Loss: 0.0690
Batch 608/1623, Avg Loss: 0.0691
Batch 640/1623, Avg Loss: 0.0691
Batch 672/1623, Avg Loss: 0.0689
Batch 704/1623, Avg Loss: 0.0691
Batch 736/1623, Avg Loss: 0.0690
Batch 768/1623, Avg Loss: 0.0692
Batch 800/1623, Avg Loss: 0.0692
Batch 832/1623, Avg Loss: 0.0690
Batch 864/1623, Avg Loss: 0.0692
Batch 896/1623, Avg Loss: 0.0690
Batch 928/1623, Avg Loss: 0.0689
Batch 960/1623, Avg Loss: 0.0689
Batch 992/1623, Avg Loss: 0.0685
Batch 1024/1623, Avg Loss: 0.0685
Batch 1056/1623, Avg Loss: 0.0686
Batch 1088/1623, Avg Loss: 0.0685
Batch 1120/1623, Avg Loss: 0.0687
Batch 1152/1623, Avg Loss: 0.0686
Batch 1184/1623, Avg Loss: 0.0687
Batch 1216/1623, Avg Loss: 0.0688
Batch 1248/1623, Avg Loss: 0.0689
Batch 1280/1623, Avg Loss: 0.0687
Batch 1312/1623, Avg Loss: 0.0687
Batch 1344/1623, Avg Loss: 0.0686
Batch 1376/1623, Avg Loss: 0.0687
Batch 1408/1623, Avg Loss: 0.0688
Batch 1440/1623, Avg Loss: 0.0686
Batch 1472/1623, Avg Loss: 0.0686
Batch 1504/1623, Avg Loss: 0.0688
Batch 1536/1623, Avg Loss: 0.0688
Batch 1568/1623, Avg Loss: 0.0689
Batch 1600/1623, Avg Loss: 0.0689
Batch 1623/1623, Avg Loss: 0.0692

Epoch 9: Train Loss: 0.0692

Val Set, val_loss: 4.4151, C-index: 0.4239
EarlyStopping counter: 8 out of 20


Batch 32/1623, Avg Loss: 0.0661
Batch 64/1623, Avg Loss: 0.0671
Batch 96/1623, Avg Loss: 0.0674
Batch 128/1623, Avg Loss: 0.0681
Batch 160/1623, Avg Loss: 0.0668
Batch 192/1623, Avg Loss: 0.0661
Batch 224/1623, Avg Loss: 0.0656
Batch 256/1623, Avg Loss: 0.0665
Batch 288/1623, Avg Loss: 0.0664
Batch 320/1623, Avg Loss: 0.0663
Batch 352/1623, Avg Loss: 0.0666
Batch 384/1623, Avg Loss: 0.0664
Batch 416/1623, Avg Loss: 0.0662
Batch 448/1623, Avg Loss: 0.0668
Batch 480/1623, Avg Loss: 0.0665
Batch 512/1623, Avg Loss: 0.0665
Batch 544/1623, Avg Loss: 0.0669
Batch 576/1623, Avg Loss: 0.0667
Batch 608/1623, Avg Loss: 0.0671
Batch 640/1623, Avg Loss: 0.0673
Batch 672/1623, Avg Loss: 0.0673
Batch 704/1623, Avg Loss: 0.0674
Batch 736/1623, Avg Loss: 0.0672
Batch 768/1623, Avg Loss: 0.0673
Batch 800/1623, Avg Loss: 0.0675
Batch 832/1623, Avg Loss: 0.0677
Batch 864/1623, Avg Loss: 0.0676
Batch 896/1623, Avg Loss: 0.0677
Batch 928/1623, Avg Loss: 0.0677
Batch 960/1623, Avg Loss: 0.0675
Batch 992/1623, Avg Loss: 0.0676
Batch 1024/1623, Avg Loss: 0.0676
Batch 1056/1623, Avg Loss: 0.0676
Batch 1088/1623, Avg Loss: 0.0672
Batch 1120/1623, Avg Loss: 0.0673
Batch 1152/1623, Avg Loss: 0.0672
Batch 1184/1623, Avg Loss: 0.0676
Batch 1216/1623, Avg Loss: 0.0677
Batch 1248/1623, Avg Loss: 0.0677
Batch 1280/1623, Avg Loss: 0.0678
Batch 1312/1623, Avg Loss: 0.0680
Batch 1344/1623, Avg Loss: 0.0680
Batch 1376/1623, Avg Loss: 0.0681
Batch 1408/1623, Avg Loss: 0.0680
Batch 1440/1623, Avg Loss: 0.0679
Batch 1472/1623, Avg Loss: 0.0679
Batch 1504/1623, Avg Loss: 0.0680
Batch 1536/1623, Avg Loss: 0.0679
Batch 1568/1623, Avg Loss: 0.0681
Batch 1600/1623, Avg Loss: 0.0682
Batch 1623/1623, Avg Loss: 0.0684

Epoch 10: Train Loss: 0.0684

Val Set, val_loss: 4.3400, C-index: 0.4082
EarlyStopping counter: 9 out of 20


Batch 32/1623, Avg Loss: 0.0651
Batch 64/1623, Avg Loss: 0.0672
Batch 96/1623, Avg Loss: 0.0681
Batch 128/1623, Avg Loss: 0.0687
Batch 160/1623, Avg Loss: 0.0672
Batch 192/1623, Avg Loss: 0.0678
Batch 224/1623, Avg Loss: 0.0678
Batch 256/1623, Avg Loss: 0.0676
Batch 288/1623, Avg Loss: 0.0666
Batch 320/1623, Avg Loss: 0.0670
Batch 352/1623, Avg Loss: 0.0671
Batch 384/1623, Avg Loss: 0.0670
Batch 416/1623, Avg Loss: 0.0669
Batch 448/1623, Avg Loss: 0.0674
Batch 480/1623, Avg Loss: 0.0675
Batch 512/1623, Avg Loss: 0.0678
Batch 544/1623, Avg Loss: 0.0680
Batch 576/1623, Avg Loss: 0.0678
Batch 608/1623, Avg Loss: 0.0678
Batch 640/1623, Avg Loss: 0.0677
Batch 672/1623, Avg Loss: 0.0676
Batch 704/1623, Avg Loss: 0.0675
Batch 736/1623, Avg Loss: 0.0673
Batch 768/1623, Avg Loss: 0.0675
Batch 800/1623, Avg Loss: 0.0678
Batch 832/1623, Avg Loss: 0.0676
Batch 864/1623, Avg Loss: 0.0674
Batch 896/1623, Avg Loss: 0.0674
Batch 928/1623, Avg Loss: 0.0674
Batch 960/1623, Avg Loss: 0.0671
Batch 992/1623, Avg Loss: 0.0673
Batch 1024/1623, Avg Loss: 0.0675
Batch 1056/1623, Avg Loss: 0.0677
Batch 1088/1623, Avg Loss: 0.0677
Batch 1120/1623, Avg Loss: 0.0678
Batch 1152/1623, Avg Loss: 0.0677
Batch 1184/1623, Avg Loss: 0.0678
Batch 1216/1623, Avg Loss: 0.0679
Batch 1248/1623, Avg Loss: 0.0679
Batch 1280/1623, Avg Loss: 0.0679
Batch 1312/1623, Avg Loss: 0.0679
Batch 1344/1623, Avg Loss: 0.0679
Batch 1376/1623, Avg Loss: 0.0680
Batch 1408/1623, Avg Loss: 0.0679
Batch 1440/1623, Avg Loss: 0.0679
Batch 1472/1623, Avg Loss: 0.0677
Batch 1504/1623, Avg Loss: 0.0677
Batch 1536/1623, Avg Loss: 0.0677
Batch 1568/1623, Avg Loss: 0.0678
Batch 1600/1623, Avg Loss: 0.0679
Batch 1623/1623, Avg Loss: 0.0683

Epoch 11: Train Loss: 0.0683

Val Set, val_loss: 4.4527, C-index: 0.4293
EarlyStopping counter: 10 out of 20


Batch 32/1623, Avg Loss: 0.0656
Batch 64/1623, Avg Loss: 0.0691
Batch 96/1623, Avg Loss: 0.0676
Batch 128/1623, Avg Loss: 0.0677
Batch 160/1623, Avg Loss: 0.0673
Batch 192/1623, Avg Loss: 0.0678
Batch 224/1623, Avg Loss: 0.0680
Batch 256/1623, Avg Loss: 0.0681
Batch 288/1623, Avg Loss: 0.0685
Batch 320/1623, Avg Loss: 0.0677
Batch 352/1623, Avg Loss: 0.0673
Batch 384/1623, Avg Loss: 0.0671
Batch 416/1623, Avg Loss: 0.0669
Batch 448/1623, Avg Loss: 0.0671
Batch 480/1623, Avg Loss: 0.0672
Batch 512/1623, Avg Loss: 0.0671
Batch 544/1623, Avg Loss: 0.0669
Batch 576/1623, Avg Loss: 0.0670
Batch 608/1623, Avg Loss: 0.0669
Batch 640/1623, Avg Loss: 0.0667
Batch 672/1623, Avg Loss: 0.0667
Batch 704/1623, Avg Loss: 0.0671
Batch 736/1623, Avg Loss: 0.0670
Batch 768/1623, Avg Loss: 0.0671
Batch 800/1623, Avg Loss: 0.0670
Batch 832/1623, Avg Loss: 0.0669
Batch 864/1623, Avg Loss: 0.0667
Batch 896/1623, Avg Loss: 0.0667
Batch 928/1623, Avg Loss: 0.0665
Batch 960/1623, Avg Loss: 0.0663
Batch 992/1623, Avg Loss: 0.0661
Batch 1024/1623, Avg Loss: 0.0662
Batch 1056/1623, Avg Loss: 0.0660
Batch 1088/1623, Avg Loss: 0.0661
Batch 1120/1623, Avg Loss: 0.0660
Batch 1152/1623, Avg Loss: 0.0660
Batch 1184/1623, Avg Loss: 0.0660
Batch 1216/1623, Avg Loss: 0.0662
Batch 1248/1623, Avg Loss: 0.0662
Batch 1280/1623, Avg Loss: 0.0661
Batch 1312/1623, Avg Loss: 0.0661
Batch 1344/1623, Avg Loss: 0.0660
Batch 1376/1623, Avg Loss: 0.0660
Batch 1408/1623, Avg Loss: 0.0658
Batch 1440/1623, Avg Loss: 0.0659
Batch 1472/1623, Avg Loss: 0.0658
Batch 1504/1623, Avg Loss: 0.0660
Batch 1536/1623, Avg Loss: 0.0660
Batch 1568/1623, Avg Loss: 0.0661
Batch 1600/1623, Avg Loss: 0.0661
Batch 1623/1623, Avg Loss: 0.0663

Epoch 12: Train Loss: 0.0663

Val Set, val_loss: 4.4778, C-index: 0.4383
EarlyStopping counter: 11 out of 20


Batch 32/1623, Avg Loss: 0.0736
Batch 64/1623, Avg Loss: 0.0719
Batch 96/1623, Avg Loss: 0.0729
Batch 128/1623, Avg Loss: 0.0703
Batch 160/1623, Avg Loss: 0.0693
Batch 192/1623, Avg Loss: 0.0686
Batch 224/1623, Avg Loss: 0.0677
Batch 256/1623, Avg Loss: 0.0677
Batch 288/1623, Avg Loss: 0.0674
Batch 320/1623, Avg Loss: 0.0671
Batch 352/1623, Avg Loss: 0.0667
Batch 384/1623, Avg Loss: 0.0673
Batch 416/1623, Avg Loss: 0.0671
Batch 448/1623, Avg Loss: 0.0670
Batch 480/1623, Avg Loss: 0.0667
Batch 512/1623, Avg Loss: 0.0668
Batch 544/1623, Avg Loss: 0.0667
Batch 576/1623, Avg Loss: 0.0659
Batch 608/1623, Avg Loss: 0.0658
Batch 640/1623, Avg Loss: 0.0656
Batch 672/1623, Avg Loss: 0.0658
Batch 704/1623, Avg Loss: 0.0659
Batch 736/1623, Avg Loss: 0.0659
Batch 768/1623, Avg Loss: 0.0662
Batch 800/1623, Avg Loss: 0.0659
Batch 832/1623, Avg Loss: 0.0659
Batch 864/1623, Avg Loss: 0.0658
Batch 896/1623, Avg Loss: 0.0658
Batch 928/1623, Avg Loss: 0.0655
Batch 960/1623, Avg Loss: 0.0654
Batch 992/1623, Avg Loss: 0.0655
Batch 1024/1623, Avg Loss: 0.0656
Batch 1056/1623, Avg Loss: 0.0656
Batch 1088/1623, Avg Loss: 0.0657
Batch 1120/1623, Avg Loss: 0.0657
Batch 1152/1623, Avg Loss: 0.0658
Batch 1184/1623, Avg Loss: 0.0657
Batch 1216/1623, Avg Loss: 0.0657
Batch 1248/1623, Avg Loss: 0.0657
Batch 1280/1623, Avg Loss: 0.0656
Batch 1312/1623, Avg Loss: 0.0656
Batch 1344/1623, Avg Loss: 0.0656
Batch 1376/1623, Avg Loss: 0.0655
Batch 1408/1623, Avg Loss: 0.0657
Batch 1440/1623, Avg Loss: 0.0656
Batch 1472/1623, Avg Loss: 0.0655
Batch 1504/1623, Avg Loss: 0.0658
Batch 1536/1623, Avg Loss: 0.0657
Batch 1568/1623, Avg Loss: 0.0658
Batch 1600/1623, Avg Loss: 0.0658
Batch 1623/1623, Avg Loss: 0.0659

Epoch 13: Train Loss: 0.0659

Val Set, val_loss: 4.6874, C-index: 0.4404
EarlyStopping counter: 12 out of 20


Batch 32/1623, Avg Loss: 0.0673
Batch 64/1623, Avg Loss: 0.0655
Batch 96/1623, Avg Loss: 0.0657
Batch 128/1623, Avg Loss: 0.0649
Batch 160/1623, Avg Loss: 0.0648
Batch 192/1623, Avg Loss: 0.0637
Batch 224/1623, Avg Loss: 0.0634
Batch 256/1623, Avg Loss: 0.0636
Batch 288/1623, Avg Loss: 0.0632
Batch 320/1623, Avg Loss: 0.0637
Batch 352/1623, Avg Loss: 0.0634
Batch 384/1623, Avg Loss: 0.0634
Batch 416/1623, Avg Loss: 0.0631
Batch 448/1623, Avg Loss: 0.0637
Batch 480/1623, Avg Loss: 0.0640
Batch 512/1623, Avg Loss: 0.0640
Batch 544/1623, Avg Loss: 0.0644
Batch 576/1623, Avg Loss: 0.0643
Batch 608/1623, Avg Loss: 0.0644
Batch 640/1623, Avg Loss: 0.0641
Batch 672/1623, Avg Loss: 0.0642
Batch 704/1623, Avg Loss: 0.0641
Batch 736/1623, Avg Loss: 0.0647
Batch 768/1623, Avg Loss: 0.0645
Batch 800/1623, Avg Loss: 0.0650
Batch 832/1623, Avg Loss: 0.0649
Batch 864/1623, Avg Loss: 0.0648
Batch 896/1623, Avg Loss: 0.0650
Batch 928/1623, Avg Loss: 0.0649
Batch 960/1623, Avg Loss: 0.0650
Batch 992/1623, Avg Loss: 0.0650
Batch 1024/1623, Avg Loss: 0.0650
Batch 1056/1623, Avg Loss: 0.0650
Batch 1088/1623, Avg Loss: 0.0651
Batch 1120/1623, Avg Loss: 0.0649
Batch 1152/1623, Avg Loss: 0.0648
Batch 1184/1623, Avg Loss: 0.0645
Batch 1216/1623, Avg Loss: 0.0643
Batch 1248/1623, Avg Loss: 0.0643
Batch 1280/1623, Avg Loss: 0.0642
Batch 1312/1623, Avg Loss: 0.0643
Batch 1344/1623, Avg Loss: 0.0644
Batch 1376/1623, Avg Loss: 0.0647
Batch 1408/1623, Avg Loss: 0.0647
Batch 1440/1623, Avg Loss: 0.0649
Batch 1472/1623, Avg Loss: 0.0648
Batch 1504/1623, Avg Loss: 0.0649
Batch 1536/1623, Avg Loss: 0.0648
Batch 1568/1623, Avg Loss: 0.0647
Batch 1600/1623, Avg Loss: 0.0648
Batch 1623/1623, Avg Loss: 0.0649

Epoch 14: Train Loss: 0.0649

Val Set, val_loss: 4.5383, C-index: 0.4286
EarlyStopping counter: 13 out of 20


Batch 32/1623, Avg Loss: 0.0583
Batch 64/1623, Avg Loss: 0.0599
Batch 96/1623, Avg Loss: 0.0595
Batch 128/1623, Avg Loss: 0.0602
Batch 160/1623, Avg Loss: 0.0602
Batch 192/1623, Avg Loss: 0.0613
Batch 224/1623, Avg Loss: 0.0621
Batch 256/1623, Avg Loss: 0.0617
Batch 288/1623, Avg Loss: 0.0626
Batch 320/1623, Avg Loss: 0.0628
Batch 352/1623, Avg Loss: 0.0629
Batch 384/1623, Avg Loss: 0.0625
Batch 416/1623, Avg Loss: 0.0625
Batch 448/1623, Avg Loss: 0.0628
Batch 480/1623, Avg Loss: 0.0621
Batch 512/1623, Avg Loss: 0.0613
Batch 544/1623, Avg Loss: 0.0615
Batch 576/1623, Avg Loss: 0.0617
Batch 608/1623, Avg Loss: 0.0615
Batch 640/1623, Avg Loss: 0.0618
Batch 672/1623, Avg Loss: 0.0619
Batch 704/1623, Avg Loss: 0.0616
Batch 736/1623, Avg Loss: 0.0616
Batch 768/1623, Avg Loss: 0.0619
Batch 800/1623, Avg Loss: 0.0621
Batch 832/1623, Avg Loss: 0.0621
Batch 864/1623, Avg Loss: 0.0621
Batch 896/1623, Avg Loss: 0.0621
Batch 928/1623, Avg Loss: 0.0622
Batch 960/1623, Avg Loss: 0.0620
Batch 992/1623, Avg Loss: 0.0620
Batch 1024/1623, Avg Loss: 0.0621
Batch 1056/1623, Avg Loss: 0.0623
Batch 1088/1623, Avg Loss: 0.0622
Batch 1120/1623, Avg Loss: 0.0622
Batch 1152/1623, Avg Loss: 0.0622
Batch 1184/1623, Avg Loss: 0.0621
Batch 1216/1623, Avg Loss: 0.0624
Batch 1248/1623, Avg Loss: 0.0624
Batch 1280/1623, Avg Loss: 0.0627
Batch 1312/1623, Avg Loss: 0.0628
Batch 1344/1623, Avg Loss: 0.0627
Batch 1376/1623, Avg Loss: 0.0628
Batch 1408/1623, Avg Loss: 0.0627
Batch 1440/1623, Avg Loss: 0.0626
Batch 1472/1623, Avg Loss: 0.0628
Batch 1504/1623, Avg Loss: 0.0630
Batch 1536/1623, Avg Loss: 0.0630
Batch 1568/1623, Avg Loss: 0.0631
Batch 1600/1623, Avg Loss: 0.0631
Batch 1623/1623, Avg Loss: 0.0633

Epoch 15: Train Loss: 0.0633

Val Set, val_loss: 4.5494, C-index: 0.4315
EarlyStopping counter: 14 out of 20


Batch 32/1623, Avg Loss: 0.0677
Batch 64/1623, Avg Loss: 0.0651
Batch 96/1623, Avg Loss: 0.0630
Batch 128/1623, Avg Loss: 0.0630
Batch 160/1623, Avg Loss: 0.0630
Batch 192/1623, Avg Loss: 0.0621
Batch 224/1623, Avg Loss: 0.0621
Batch 256/1623, Avg Loss: 0.0616
Batch 288/1623, Avg Loss: 0.0621
Batch 320/1623, Avg Loss: 0.0620
Batch 352/1623, Avg Loss: 0.0620
Batch 384/1623, Avg Loss: 0.0616
Batch 416/1623, Avg Loss: 0.0618
Batch 448/1623, Avg Loss: 0.0617
Batch 480/1623, Avg Loss: 0.0616
Batch 512/1623, Avg Loss: 0.0612
Batch 544/1623, Avg Loss: 0.0612
Batch 576/1623, Avg Loss: 0.0615
Batch 608/1623, Avg Loss: 0.0616
Batch 640/1623, Avg Loss: 0.0617
Batch 672/1623, Avg Loss: 0.0613
Batch 704/1623, Avg Loss: 0.0613
Batch 736/1623, Avg Loss: 0.0615
Batch 768/1623, Avg Loss: 0.0618
Batch 800/1623, Avg Loss: 0.0616
Batch 832/1623, Avg Loss: 0.0618
Batch 864/1623, Avg Loss: 0.0620
Batch 896/1623, Avg Loss: 0.0620
Batch 928/1623, Avg Loss: 0.0620
Batch 960/1623, Avg Loss: 0.0620
Batch 992/1623, Avg Loss: 0.0619
Batch 1024/1623, Avg Loss: 0.0618
Batch 1056/1623, Avg Loss: 0.0618
Batch 1088/1623, Avg Loss: 0.0617
Batch 1120/1623, Avg Loss: 0.0620
Batch 1152/1623, Avg Loss: 0.0621
Batch 1184/1623, Avg Loss: 0.0621
Batch 1216/1623, Avg Loss: 0.0620
Batch 1248/1623, Avg Loss: 0.0621
Batch 1280/1623, Avg Loss: 0.0620
Batch 1312/1623, Avg Loss: 0.0621
Batch 1344/1623, Avg Loss: 0.0622
Batch 1376/1623, Avg Loss: 0.0623
Batch 1408/1623, Avg Loss: 0.0622
Batch 1440/1623, Avg Loss: 0.0622
Batch 1472/1623, Avg Loss: 0.0624
Batch 1504/1623, Avg Loss: 0.0624
Batch 1536/1623, Avg Loss: 0.0622
Batch 1568/1623, Avg Loss: 0.0625
Batch 1600/1623, Avg Loss: 0.0624
Batch 1623/1623, Avg Loss: 0.0625

Epoch 16: Train Loss: 0.0625

Val Set, val_loss: 4.5376, C-index: 0.4267
EarlyStopping counter: 15 out of 20


Batch 32/1623, Avg Loss: 0.0564
Batch 64/1623, Avg Loss: 0.0608
Batch 96/1623, Avg Loss: 0.0617
Batch 128/1623, Avg Loss: 0.0634
Batch 160/1623, Avg Loss: 0.0626
Batch 192/1623, Avg Loss: 0.0621
Batch 224/1623, Avg Loss: 0.0619
Batch 256/1623, Avg Loss: 0.0621
Batch 288/1623, Avg Loss: 0.0626
Batch 320/1623, Avg Loss: 0.0622
Batch 352/1623, Avg Loss: 0.0624
Batch 384/1623, Avg Loss: 0.0618
Batch 416/1623, Avg Loss: 0.0615
Batch 448/1623, Avg Loss: 0.0616
Batch 480/1623, Avg Loss: 0.0615
Batch 512/1623, Avg Loss: 0.0611
Batch 544/1623, Avg Loss: 0.0608
Batch 576/1623, Avg Loss: 0.0610
Batch 608/1623, Avg Loss: 0.0613
Batch 640/1623, Avg Loss: 0.0610
Batch 672/1623, Avg Loss: 0.0606
Batch 704/1623, Avg Loss: 0.0610
Batch 736/1623, Avg Loss: 0.0610
Batch 768/1623, Avg Loss: 0.0610
Batch 800/1623, Avg Loss: 0.0608
Batch 832/1623, Avg Loss: 0.0606
Batch 864/1623, Avg Loss: 0.0608
Batch 896/1623, Avg Loss: 0.0609
Batch 928/1623, Avg Loss: 0.0607
Batch 960/1623, Avg Loss: 0.0608
Batch 992/1623, Avg Loss: 0.0609
Batch 1024/1623, Avg Loss: 0.0610
Batch 1056/1623, Avg Loss: 0.0612
Batch 1088/1623, Avg Loss: 0.0612
Batch 1120/1623, Avg Loss: 0.0614
Batch 1152/1623, Avg Loss: 0.0612
Batch 1184/1623, Avg Loss: 0.0612
Batch 1216/1623, Avg Loss: 0.0614
Batch 1248/1623, Avg Loss: 0.0615
Batch 1280/1623, Avg Loss: 0.0615
Batch 1312/1623, Avg Loss: 0.0616
Batch 1344/1623, Avg Loss: 0.0619
Batch 1376/1623, Avg Loss: 0.0620
Batch 1408/1623, Avg Loss: 0.0618
Batch 1440/1623, Avg Loss: 0.0619
Batch 1472/1623, Avg Loss: 0.0620
Batch 1504/1623, Avg Loss: 0.0620
Batch 1536/1623, Avg Loss: 0.0619
Batch 1568/1623, Avg Loss: 0.0619
Batch 1600/1623, Avg Loss: 0.0618
Batch 1623/1623, Avg Loss: 0.0619

Epoch 17: Train Loss: 0.0619

Val Set, val_loss: 4.6106, C-index: 0.4357
EarlyStopping counter: 16 out of 20


Batch 32/1623, Avg Loss: 0.0598
Batch 64/1623, Avg Loss: 0.0596
Batch 96/1623, Avg Loss: 0.0605
Batch 128/1623, Avg Loss: 0.0616
Batch 160/1623, Avg Loss: 0.0623
Batch 192/1623, Avg Loss: 0.0617
Batch 224/1623, Avg Loss: 0.0607
Batch 256/1623, Avg Loss: 0.0605
Batch 288/1623, Avg Loss: 0.0607
Batch 320/1623, Avg Loss: 0.0604
Batch 352/1623, Avg Loss: 0.0600
Batch 384/1623, Avg Loss: 0.0599
Batch 416/1623, Avg Loss: 0.0599
Batch 448/1623, Avg Loss: 0.0599
Batch 480/1623, Avg Loss: 0.0599
Batch 512/1623, Avg Loss: 0.0595
Batch 544/1623, Avg Loss: 0.0596
Batch 576/1623, Avg Loss: 0.0599
Batch 608/1623, Avg Loss: 0.0598
Batch 640/1623, Avg Loss: 0.0600
Batch 672/1623, Avg Loss: 0.0601
Batch 704/1623, Avg Loss: 0.0603
Batch 736/1623, Avg Loss: 0.0606
Batch 768/1623, Avg Loss: 0.0603
Batch 800/1623, Avg Loss: 0.0601
Batch 832/1623, Avg Loss: 0.0599
Batch 864/1623, Avg Loss: 0.0598
Batch 896/1623, Avg Loss: 0.0596
Batch 928/1623, Avg Loss: 0.0598
Batch 960/1623, Avg Loss: 0.0600
Batch 992/1623, Avg Loss: 0.0600
Batch 1024/1623, Avg Loss: 0.0600
Batch 1056/1623, Avg Loss: 0.0600
Batch 1088/1623, Avg Loss: 0.0602
Batch 1120/1623, Avg Loss: 0.0603
Batch 1152/1623, Avg Loss: 0.0603
Batch 1184/1623, Avg Loss: 0.0604
Batch 1216/1623, Avg Loss: 0.0605
Batch 1248/1623, Avg Loss: 0.0607
Batch 1280/1623, Avg Loss: 0.0607
Batch 1312/1623, Avg Loss: 0.0606
Batch 1344/1623, Avg Loss: 0.0608
Batch 1376/1623, Avg Loss: 0.0607
Batch 1408/1623, Avg Loss: 0.0606
Batch 1440/1623, Avg Loss: 0.0607
Batch 1472/1623, Avg Loss: 0.0607
Batch 1504/1623, Avg Loss: 0.0607
Batch 1536/1623, Avg Loss: 0.0607
Batch 1568/1623, Avg Loss: 0.0606
Batch 1600/1623, Avg Loss: 0.0606
Batch 1623/1623, Avg Loss: 0.0609

Epoch 18: Train Loss: 0.0609

Val Set, val_loss: 4.8339, C-index: 0.4240
EarlyStopping counter: 17 out of 20


Batch 32/1623, Avg Loss: 0.0618
Batch 64/1623, Avg Loss: 0.0571
Batch 96/1623, Avg Loss: 0.0575
Batch 128/1623, Avg Loss: 0.0591
Batch 160/1623, Avg Loss: 0.0594
Batch 192/1623, Avg Loss: 0.0588
Batch 224/1623, Avg Loss: 0.0588
Batch 256/1623, Avg Loss: 0.0590
Batch 288/1623, Avg Loss: 0.0586
Batch 320/1623, Avg Loss: 0.0587
Batch 352/1623, Avg Loss: 0.0581
Batch 384/1623, Avg Loss: 0.0582
Batch 416/1623, Avg Loss: 0.0587
Batch 448/1623, Avg Loss: 0.0584
Batch 480/1623, Avg Loss: 0.0586
Batch 512/1623, Avg Loss: 0.0586
Batch 544/1623, Avg Loss: 0.0593
Batch 576/1623, Avg Loss: 0.0596
Batch 608/1623, Avg Loss: 0.0599
Batch 640/1623, Avg Loss: 0.0598
Batch 672/1623, Avg Loss: 0.0601
Batch 704/1623, Avg Loss: 0.0599
Batch 736/1623, Avg Loss: 0.0601
Batch 768/1623, Avg Loss: 0.0602
Batch 800/1623, Avg Loss: 0.0597
Batch 832/1623, Avg Loss: 0.0598
Batch 864/1623, Avg Loss: 0.0596
Batch 896/1623, Avg Loss: 0.0596
Batch 928/1623, Avg Loss: 0.0595
Batch 960/1623, Avg Loss: 0.0597
Batch 992/1623, Avg Loss: 0.0595
Batch 1024/1623, Avg Loss: 0.0596
Batch 1056/1623, Avg Loss: 0.0596
Batch 1088/1623, Avg Loss: 0.0597
Batch 1120/1623, Avg Loss: 0.0594
Batch 1152/1623, Avg Loss: 0.0593
Batch 1184/1623, Avg Loss: 0.0591
Batch 1216/1623, Avg Loss: 0.0589
Batch 1248/1623, Avg Loss: 0.0590
Batch 1280/1623, Avg Loss: 0.0591
Batch 1312/1623, Avg Loss: 0.0590
Batch 1344/1623, Avg Loss: 0.0588
Batch 1376/1623, Avg Loss: 0.0589
Batch 1408/1623, Avg Loss: 0.0589
Batch 1440/1623, Avg Loss: 0.0587
Batch 1472/1623, Avg Loss: 0.0586
Batch 1504/1623, Avg Loss: 0.0585
Batch 1536/1623, Avg Loss: 0.0586
Batch 1568/1623, Avg Loss: 0.0584
Batch 1600/1623, Avg Loss: 0.0585
Batch 1623/1623, Avg Loss: 0.0589

Epoch 19: Train Loss: 0.0589

Val Set, val_loss: 4.7530, C-index: 0.4131
EarlyStopping counter: 18 out of 20


Batch 32/1623, Avg Loss: 0.0539
Batch 64/1623, Avg Loss: 0.0542
Batch 96/1623, Avg Loss: 0.0548
Batch 128/1623, Avg Loss: 0.0542
Batch 160/1623, Avg Loss: 0.0539
Batch 192/1623, Avg Loss: 0.0544
Batch 224/1623, Avg Loss: 0.0553
Batch 256/1623, Avg Loss: 0.0557
Batch 288/1623, Avg Loss: 0.0564
Batch 320/1623, Avg Loss: 0.0564
Batch 352/1623, Avg Loss: 0.0568
Batch 384/1623, Avg Loss: 0.0568
Batch 416/1623, Avg Loss: 0.0567
Batch 448/1623, Avg Loss: 0.0569
Batch 480/1623, Avg Loss: 0.0565
Batch 512/1623, Avg Loss: 0.0568
Batch 544/1623, Avg Loss: 0.0571
Batch 576/1623, Avg Loss: 0.0574
Batch 608/1623, Avg Loss: 0.0574
Batch 640/1623, Avg Loss: 0.0576
Batch 672/1623, Avg Loss: 0.0574
Batch 704/1623, Avg Loss: 0.0577
Batch 736/1623, Avg Loss: 0.0578
Batch 768/1623, Avg Loss: 0.0578
Batch 800/1623, Avg Loss: 0.0579
Batch 832/1623, Avg Loss: 0.0578
Batch 864/1623, Avg Loss: 0.0580
Batch 896/1623, Avg Loss: 0.0579
Batch 928/1623, Avg Loss: 0.0583
Batch 960/1623, Avg Loss: 0.0586
Batch 992/1623, Avg Loss: 0.0587
Batch 1024/1623, Avg Loss: 0.0589
Batch 1056/1623, Avg Loss: 0.0589
Batch 1088/1623, Avg Loss: 0.0589
Batch 1120/1623, Avg Loss: 0.0589
Batch 1152/1623, Avg Loss: 0.0589
Batch 1184/1623, Avg Loss: 0.0587
Batch 1216/1623, Avg Loss: 0.0587
Batch 1248/1623, Avg Loss: 0.0588
Batch 1280/1623, Avg Loss: 0.0587
Batch 1312/1623, Avg Loss: 0.0587
Batch 1344/1623, Avg Loss: 0.0587
Batch 1376/1623, Avg Loss: 0.0587
Batch 1408/1623, Avg Loss: 0.0587
Batch 1440/1623, Avg Loss: 0.0587
Batch 1472/1623, Avg Loss: 0.0587
Batch 1504/1623, Avg Loss: 0.0587
Batch 1536/1623, Avg Loss: 0.0588
Batch 1568/1623, Avg Loss: 0.0587
Batch 1600/1623, Avg Loss: 0.0587
Batch 1623/1623, Avg Loss: 0.0588

Epoch 20: Train Loss: 0.0588

Val Set, val_loss: 4.7457, C-index: 0.4286
EarlyStopping counter: 19 out of 20


Batch 32/1623, Avg Loss: 0.0614
Batch 64/1623, Avg Loss: 0.0604
Batch 96/1623, Avg Loss: 0.0595
Batch 128/1623, Avg Loss: 0.0589
Batch 160/1623, Avg Loss: 0.0584
Batch 192/1623, Avg Loss: 0.0578
Batch 224/1623, Avg Loss: 0.0584
Batch 256/1623, Avg Loss: 0.0586
Batch 288/1623, Avg Loss: 0.0581
Batch 320/1623, Avg Loss: 0.0582
Batch 352/1623, Avg Loss: 0.0583
Batch 384/1623, Avg Loss: 0.0580
Batch 416/1623, Avg Loss: 0.0573
Batch 448/1623, Avg Loss: 0.0577
Batch 480/1623, Avg Loss: 0.0574
Batch 512/1623, Avg Loss: 0.0578
Batch 544/1623, Avg Loss: 0.0575
Batch 576/1623, Avg Loss: 0.0575
Batch 608/1623, Avg Loss: 0.0575
Batch 640/1623, Avg Loss: 0.0576
Batch 672/1623, Avg Loss: 0.0579
Batch 704/1623, Avg Loss: 0.0582
Batch 736/1623, Avg Loss: 0.0576
Batch 768/1623, Avg Loss: 0.0572
Batch 800/1623, Avg Loss: 0.0576
Batch 832/1623, Avg Loss: 0.0578
Batch 864/1623, Avg Loss: 0.0582
Batch 896/1623, Avg Loss: 0.0579
Batch 928/1623, Avg Loss: 0.0580
Batch 960/1623, Avg Loss: 0.0578
Batch 992/1623, Avg Loss: 0.0577
Batch 1024/1623, Avg Loss: 0.0578
Batch 1056/1623, Avg Loss: 0.0577
Batch 1088/1623, Avg Loss: 0.0577
Batch 1120/1623, Avg Loss: 0.0577
Batch 1152/1623, Avg Loss: 0.0576
Batch 1184/1623, Avg Loss: 0.0576
Batch 1216/1623, Avg Loss: 0.0576
Batch 1248/1623, Avg Loss: 0.0576
Batch 1280/1623, Avg Loss: 0.0577
Batch 1312/1623, Avg Loss: 0.0576
Batch 1344/1623, Avg Loss: 0.0576
Batch 1376/1623, Avg Loss: 0.0578
Batch 1408/1623, Avg Loss: 0.0576
Batch 1440/1623, Avg Loss: 0.0575
Batch 1472/1623, Avg Loss: 0.0576
Batch 1504/1623, Avg Loss: 0.0575
Batch 1536/1623, Avg Loss: 0.0577
Batch 1568/1623, Avg Loss: 0.0577
Batch 1600/1623, Avg Loss: 0.0579
Batch 1623/1623, Avg Loss: 0.0579

Epoch 21: Train Loss: 0.0579

Val Set, val_loss: 4.8643, C-index: 0.4193
EarlyStopping counter: 20 out of 20


Batch 32/1623, Avg Loss: 0.0567
Batch 64/1623, Avg Loss: 0.0615
Batch 96/1623, Avg Loss: 0.0624
Batch 128/1623, Avg Loss: 0.0606
Batch 160/1623, Avg Loss: 0.0613
Batch 192/1623, Avg Loss: 0.0608
Batch 224/1623, Avg Loss: 0.0601
Batch 256/1623, Avg Loss: 0.0596
Batch 288/1623, Avg Loss: 0.0597
Batch 320/1623, Avg Loss: 0.0586
Batch 352/1623, Avg Loss: 0.0591
Batch 384/1623, Avg Loss: 0.0587
Batch 416/1623, Avg Loss: 0.0587
Batch 448/1623, Avg Loss: 0.0585
Batch 480/1623, Avg Loss: 0.0584
Batch 512/1623, Avg Loss: 0.0587
Batch 544/1623, Avg Loss: 0.0585
Batch 576/1623, Avg Loss: 0.0588
Batch 608/1623, Avg Loss: 0.0590
Batch 640/1623, Avg Loss: 0.0589
Batch 672/1623, Avg Loss: 0.0587
Batch 704/1623, Avg Loss: 0.0589
Batch 736/1623, Avg Loss: 0.0587
Batch 768/1623, Avg Loss: 0.0583
Batch 800/1623, Avg Loss: 0.0582
Batch 832/1623, Avg Loss: 0.0578
Batch 864/1623, Avg Loss: 0.0581
Batch 896/1623, Avg Loss: 0.0579
Batch 928/1623, Avg Loss: 0.0578
Batch 960/1623, Avg Loss: 0.0579
Batch 992/1623, Avg Loss: 0.0580
Batch 1024/1623, Avg Loss: 0.0576
Batch 1056/1623, Avg Loss: 0.0574
Batch 1088/1623, Avg Loss: 0.0574
Batch 1120/1623, Avg Loss: 0.0573
Batch 1152/1623, Avg Loss: 0.0573
Batch 1184/1623, Avg Loss: 0.0574
Batch 1216/1623, Avg Loss: 0.0573
Batch 1248/1623, Avg Loss: 0.0575
Batch 1280/1623, Avg Loss: 0.0575
Batch 1312/1623, Avg Loss: 0.0575
Batch 1344/1623, Avg Loss: 0.0575
Batch 1376/1623, Avg Loss: 0.0573
Batch 1408/1623, Avg Loss: 0.0573
Batch 1440/1623, Avg Loss: 0.0571
Batch 1472/1623, Avg Loss: 0.0570
Batch 1504/1623, Avg Loss: 0.0570
Batch 1536/1623, Avg Loss: 0.0570
Batch 1568/1623, Avg Loss: 0.0570
Batch 1600/1623, Avg Loss: 0.0571
Batch 1623/1623, Avg Loss: 0.0575

Epoch 22: Train Loss: 0.0575

Val Set, val_loss: 4.9156, C-index: 0.4079
EarlyStopping counter: 21 out of 20


Batch 32/1623, Avg Loss: 0.0607
Batch 64/1623, Avg Loss: 0.0578
Batch 96/1623, Avg Loss: 0.0614
Batch 128/1623, Avg Loss: 0.0613
Batch 160/1623, Avg Loss: 0.0588
Batch 192/1623, Avg Loss: 0.0581
Batch 224/1623, Avg Loss: 0.0575
Batch 256/1623, Avg Loss: 0.0577
Batch 288/1623, Avg Loss: 0.0567
Batch 320/1623, Avg Loss: 0.0569
Batch 352/1623, Avg Loss: 0.0572
Batch 384/1623, Avg Loss: 0.0571
Batch 416/1623, Avg Loss: 0.0573
Batch 448/1623, Avg Loss: 0.0572
Batch 480/1623, Avg Loss: 0.0572
Batch 512/1623, Avg Loss: 0.0569
Batch 544/1623, Avg Loss: 0.0570
Batch 576/1623, Avg Loss: 0.0566
Batch 608/1623, Avg Loss: 0.0566
Batch 640/1623, Avg Loss: 0.0566
Batch 672/1623, Avg Loss: 0.0564
Batch 704/1623, Avg Loss: 0.0563
Batch 736/1623, Avg Loss: 0.0563
Batch 768/1623, Avg Loss: 0.0562
Batch 800/1623, Avg Loss: 0.0564
Batch 832/1623, Avg Loss: 0.0563
Batch 864/1623, Avg Loss: 0.0562
Batch 896/1623, Avg Loss: 0.0563
Batch 928/1623, Avg Loss: 0.0559
Batch 960/1623, Avg Loss: 0.0558
Batch 992/1623, Avg Loss: 0.0558
Batch 1024/1623, Avg Loss: 0.0559
Batch 1056/1623, Avg Loss: 0.0560
Batch 1088/1623, Avg Loss: 0.0563
Batch 1120/1623, Avg Loss: 0.0562
Batch 1152/1623, Avg Loss: 0.0564
Batch 1184/1623, Avg Loss: 0.0562
Batch 1216/1623, Avg Loss: 0.0562
Batch 1248/1623, Avg Loss: 0.0563
Batch 1280/1623, Avg Loss: 0.0562
Batch 1312/1623, Avg Loss: 0.0561
Batch 1344/1623, Avg Loss: 0.0563
Batch 1376/1623, Avg Loss: 0.0564
Batch 1408/1623, Avg Loss: 0.0566
Batch 1440/1623, Avg Loss: 0.0567
Batch 1472/1623, Avg Loss: 0.0566
Batch 1504/1623, Avg Loss: 0.0567
Batch 1536/1623, Avg Loss: 0.0568
Batch 1568/1623, Avg Loss: 0.0568
Batch 1600/1623, Avg Loss: 0.0569
Batch 1623/1623, Avg Loss: 0.0571

Epoch 23: Train Loss: 0.0571

Val Set, val_loss: 4.8509, C-index: 0.4170
EarlyStopping counter: 22 out of 20


Batch 32/1623, Avg Loss: 0.0503
Batch 64/1623, Avg Loss: 0.0537
Batch 96/1623, Avg Loss: 0.0559
Batch 128/1623, Avg Loss: 0.0573
Batch 160/1623, Avg Loss: 0.0575
Batch 192/1623, Avg Loss: 0.0554
Batch 224/1623, Avg Loss: 0.0562
Batch 256/1623, Avg Loss: 0.0560
Batch 288/1623, Avg Loss: 0.0555
Batch 320/1623, Avg Loss: 0.0564
Batch 352/1623, Avg Loss: 0.0565
Batch 384/1623, Avg Loss: 0.0566
Batch 416/1623, Avg Loss: 0.0562
Batch 448/1623, Avg Loss: 0.0565
Batch 480/1623, Avg Loss: 0.0564
Batch 512/1623, Avg Loss: 0.0567
Batch 544/1623, Avg Loss: 0.0571
Batch 576/1623, Avg Loss: 0.0569
Batch 608/1623, Avg Loss: 0.0569
Batch 640/1623, Avg Loss: 0.0573
Batch 672/1623, Avg Loss: 0.0573
Batch 704/1623, Avg Loss: 0.0573
Batch 736/1623, Avg Loss: 0.0572
Batch 768/1623, Avg Loss: 0.0574
Batch 800/1623, Avg Loss: 0.0577
Batch 832/1623, Avg Loss: 0.0577
Batch 864/1623, Avg Loss: 0.0576
Batch 896/1623, Avg Loss: 0.0576
Batch 928/1623, Avg Loss: 0.0575
Batch 960/1623, Avg Loss: 0.0576
Batch 992/1623, Avg Loss: 0.0576
Batch 1024/1623, Avg Loss: 0.0575
Batch 1056/1623, Avg Loss: 0.0573
Batch 1088/1623, Avg Loss: 0.0572
Batch 1120/1623, Avg Loss: 0.0572
Batch 1152/1623, Avg Loss: 0.0571
Batch 1184/1623, Avg Loss: 0.0574
Batch 1216/1623, Avg Loss: 0.0571
Batch 1248/1623, Avg Loss: 0.0569
Batch 1280/1623, Avg Loss: 0.0568
Batch 1312/1623, Avg Loss: 0.0568
Batch 1344/1623, Avg Loss: 0.0567
Batch 1376/1623, Avg Loss: 0.0567
Batch 1408/1623, Avg Loss: 0.0564
Batch 1440/1623, Avg Loss: 0.0566
Batch 1472/1623, Avg Loss: 0.0567
Batch 1504/1623, Avg Loss: 0.0566
Batch 1536/1623, Avg Loss: 0.0564
Batch 1568/1623, Avg Loss: 0.0563
Batch 1600/1623, Avg Loss: 0.0560
Batch 1623/1623, Avg Loss: 0.0562

Epoch 24: Train Loss: 0.0562

Val Set, val_loss: 5.1088, C-index: 0.4241
EarlyStopping counter: 23 out of 20


